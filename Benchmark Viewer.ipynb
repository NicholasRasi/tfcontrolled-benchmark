{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark Viewer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### System\n",
    "- [overhead](#Overhead)\n",
    "\n",
    "### Controller\n",
    "- [variable SLA](#Variable-SLA)\n",
    "- [variable input](#Variable-Input)\n",
    "- [variable workload](#Variable-Workload)\n",
    "\n",
    "### System\n",
    "- [variable reqs/s](#System-Variable-reqs/s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "import statistics as stat\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = [10, 8]\n",
    "cmap = plt.get_cmap('jet_r')\n",
    "font = {'family' : 'DejaVu Sans',\n",
    "        'weight' : 'normal',\n",
    "        'size'   : 18}\n",
    "plt.rc('font', **font)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_profiling_overhead(file, aggregate=False):\n",
    "    with open(file, 'rb') as f:\n",
    "        profiling_data = pickle.load(f)\n",
    "    profiled_rt_data = profiling_data[0]\n",
    "    # print(profiled_rt_data)\n",
    "    \n",
    "    for i, profiled_rt in enumerate(profiled_rt_data):\n",
    "        \n",
    "        if aggregate:\n",
    "            profiled_rt_data_aggregate = []\n",
    "            for j in range(0, len(profiled_rt), aggregate):\n",
    "                mean = stat.mean(profiled_rt[j:j+aggregate])\n",
    "                for _ in range(0, aggregate):\n",
    "                    profiled_rt_data_aggregate.append(mean)\n",
    "            profiled_rt = profiled_rt_data_aggregate\n",
    "        \n",
    "        x_val = np.arange(len(profiled_rt))\n",
    "        plt.xlabel('Req')\n",
    "        plt.ylabel('Response time [s]')\n",
    "        # plt.title('Profiled Response time')\n",
    "        plt.plot(x_val, profiled_rt, label=\"Data \" + str(i+1))\n",
    "\n",
    "        avg_rt = stat.mean(profiled_rt)\n",
    "        min_rt = min(profiled_rt)\n",
    "        max_rt = max(profiled_rt)\n",
    "        dev_rt = stat.variance(profiled_rt)\n",
    "        print(\"Min: {:.4f}, Max: {:.4f}, Avg: {:.4f}, Std: {:.4f}\".format(min_rt, max_rt, avg_rt, dev_rt))\n",
    "    \n",
    "    plt.legend(loc=1)\n",
    "    plt.grid(True, linewidth=0.3, linestyle='-')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def show_benchmark_overhead(files, aggregate=False):\n",
    "    for i, file in enumerate(files):\n",
    "        with open(file, 'rb') as f:\n",
    "            benchmark_data = pickle.load(f)\n",
    "\n",
    "        sampling_time = 2\n",
    "        benchmark_rt = list(filter(lambda rt: rt is not None, benchmark_data[0]))\n",
    "        benchmark_rt_process = list(filter(lambda rt: rt is not None, benchmark_data[1]))\n",
    "\n",
    "        avg_rt = stat.mean(benchmark_rt)\n",
    "        min_rt = min(benchmark_rt)\n",
    "        max_rt = max(benchmark_rt)\n",
    "        dev_rt = stat.variance(benchmark_rt)\n",
    "        print(\"Min: {:.4f}, Max: {:.4f}, Avg: {:.4f}, Std: {:.4f}\".format(min_rt, max_rt, avg_rt, dev_rt))\n",
    "        avg_rt = stat.mean(benchmark_rt_process)\n",
    "        min_rt = min(benchmark_rt_process)\n",
    "        max_rt = max(benchmark_rt_process)\n",
    "        dev_rt = stat.variance(benchmark_rt)\n",
    "        print(\"PROCESS: Min: {:.4f}, Max: {:.4f}, Avg: {:.4f}, Std: {:.4f}\\n\".format(min_rt, max_rt, avg_rt, dev_rt))\n",
    "\n",
    "        if aggregate:\n",
    "            benchmark_rt_aggregate = []\n",
    "            for j in range(0, len(benchmark_rt), aggregate):\n",
    "                mean = stat.mean(benchmark_rt[j:j+aggregate])\n",
    "                for _ in range(0, aggregate):\n",
    "                    benchmark_rt_aggregate.append(mean)\n",
    "            benchmark_rt = benchmark_rt_aggregate\n",
    "        \n",
    "        # plot\n",
    "        x_val = np.arange(len(benchmark_rt))\n",
    "        plt.xlabel('Time [s]')\n",
    "        plt.ylabel('Response time [s]')\n",
    "        # plt.title('Response Time')\n",
    "        plt.plot(x_val*sampling_time, benchmark_rt, label=\"Data \" + str(i+1))\n",
    "        # plt.plot(x_val*sampling_time, [stat.mean(benchmark_rt)]*len(x_val), label=\"avg RT\")\n",
    "    plt.legend(loc=1)\n",
    "    plt.grid(True, linewidth=0.3, linestyle='-')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overhead\n",
    "- CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "base_folder = \"overhead/\"\n",
    "print(\"--- PROFILING ---\")\n",
    "for model in [\"googlenet\", \"resnet_NHWC\", \"alexnet\", \"vgg16\", \"skyline_extraction\"]:\n",
    "    print(model)\n",
    "    show_profiling_overhead(base_folder + \"profiling_\" + model + \".out\", aggregate=2)\n",
    "\n",
    "print(\"--- BENCHMARK ---\")\n",
    "load_sizes = [\"small\", \"medium\", \"large\"]\n",
    "print(\"resnet_NHWC\")\n",
    "show_benchmark_overhead([base_folder + \"benchmark_resnet_NHWC_\" + load_size + \".out\" for load_size in load_sizes], aggregate=2)\n",
    "print(\"googlenet\")\n",
    "show_benchmark_overhead([base_folder + \"benchmark_googlenet_\" + load_size + \".out\" for load_size in load_sizes], aggregate=2)\n",
    "print(\"skyline\")\n",
    "show_benchmark_overhead([base_folder + \"benchmark_skyline_extraction_\" + load_size + \".out\" for load_size in load_sizes], aggregate=2)\n",
    "print(\"vgg16\")\n",
    "show_benchmark_overhead([base_folder + \"benchmark_vgg16_\" + load_size + \".out\" for load_size in load_sizes], aggregate=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Controller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def show_values(file, sampling_time, title=False, verbose=False, aggregate=False, sla=1.1):\n",
    "    with open(file, 'rb') as f:\n",
    "        benchmark_data = pickle.load(f)\n",
    "\n",
    "    benchmark_rt = list(filter(lambda rt: rt is not None, benchmark_data[0]))\n",
    "    benchmark_rt_process = list(filter(lambda rt: rt is not None, benchmark_data[1]))\n",
    "    benchmark_req = benchmark_data[2]\n",
    "    benchmark_sent = benchmark_data[3]\n",
    "    for i,_ in enumerate(benchmark_sent):\n",
    "        for j,_ in enumerate(benchmark_sent[i]):\n",
    "            benchmark_sent[i][j] = benchmark_sent[i][j] / sampling_time\n",
    "    benchmark_model_sla = list(map(lambda v: v*sla, benchmark_data[4]))\n",
    "    benchmark_containers = benchmark_data[5]\n",
    "\n",
    "    bc_json = [response.json() for response in benchmark_containers]\n",
    "    benchmark_container_quota = {}\n",
    "    for container in bc_json[0]:\n",
    "        benchmark_container_quota[container[\"container_id\"]] = []\n",
    "    for bc in bc_json:\n",
    "        for sampled_c in bc:\n",
    "            benchmark_container_quota[sampled_c[\"container_id\"]].append(sampled_c[\"quota\"]/100000)\n",
    "            \n",
    "            \n",
    "    # aggregate values\n",
    "    if aggregate:\n",
    "        benchmark_aggregate = []\n",
    "        for i in range(0, len(benchmark_rt), aggregate):\n",
    "            mean = stat.mean(benchmark_rt[i:i+aggregate])\n",
    "            for _ in range(0, aggregate):\n",
    "                benchmark_aggregate.append(mean)\n",
    "        benchmark_rt = benchmark_aggregate\n",
    "        \n",
    "    if aggregate:\n",
    "        for container in benchmark_container_quota:\n",
    "            benchmark_aggregate = []\n",
    "            for i in range(0, len(benchmark_container_quota[container]), aggregate):\n",
    "                mean = stat.mean(benchmark_container_quota[container][i:i+aggregate])\n",
    "                for _ in range(0, aggregate):\n",
    "                    benchmark_aggregate.append(mean)            \n",
    "            benchmark_container_quota[container] = benchmark_aggregate\n",
    "\n",
    "    if verbose:\n",
    "        print(\"avgs rt: \", benchmark_rt)\n",
    "        print(\"avg rt: \", stat.mean(benchmark_rt))\n",
    "        print(\"req: \", benchmark_req)\n",
    "        print(\"req sent: \", benchmark_sent)\n",
    "        print(\"model_sla: \", benchmark_model_sla)\n",
    "        # print(\"containers: \", benchmark_containers)\n",
    "        print(\"containers_quota: \", benchmark_container_quota)\n",
    "        \n",
    "    # plot\n",
    "    x_val = np.arange(len(benchmark_rt))\n",
    "    while len(benchmark_model_sla) < len(x_val):\n",
    "        benchmark_model_sla.append(0)\n",
    "    # slas = set(benchmark_model_sla)\n",
    "    plt.xlabel('Time [s]')\n",
    "    plt.ylabel('Response time [s]')\n",
    "    if title:\n",
    "        plt.title('Response Time')\n",
    "    \n",
    "    plt.plot(x_val*sampling_time, benchmark_model_sla[:len(benchmark_rt)], ':', label=\"Model SLA\", linewidth=3)\n",
    "    plt.plot(x_val*sampling_time, benchmark_rt, label=\"Response Time\", linewidth=3)\n",
    "    # plt.plot(x_val*sampling_time, [stat.mean(benchmark_rt)]*len(benchmark_rt), '--', label=\"Avg RT\")\n",
    "    plt.legend(loc=1)\n",
    "    plt.grid(True, linewidth=0.3, linestyle='-')\n",
    "    plt.show()\n",
    "\n",
    "    x_val = np.arange(len(benchmark_req))\n",
    "    while len(benchmark_model_sla) < len(x_val):\n",
    "        benchmark_model_sla.append(0)\n",
    "    plt.xlabel('time [s]')\n",
    "    plt.ylabel('# requests (completed + created)')\n",
    "    if title:\n",
    "        plt.title('# requests')\n",
    "    plt.plot(x_val*sampling_time, benchmark_req, linewidth=3)\n",
    "    plt.grid(True, linewidth=0.3, linestyle='-')\n",
    "    plt.show()\n",
    "\n",
    "    plt.xlabel('Time [s]')\n",
    "    plt.ylabel('Load [r/s]')\n",
    "    if title:\n",
    "        plt.title('Workload')\n",
    "    lines = plt.plot(x_val*sampling_time, benchmark_sent, linewidth=3)\n",
    "    labels = []\n",
    "    for l, _ in enumerate(benchmark_sent[0]):\n",
    "        labels.append(\"load \" + str(l+1))\n",
    "    plt.legend(iter(lines), labels, loc=1)\n",
    "    plt.grid(True, linewidth=0.3, linestyle='-')\n",
    "    plt.show()\n",
    "\n",
    "    plt.xlabel('Time [s]')\n",
    "    plt.ylabel('SLA [s]')\n",
    "    if title:\n",
    "        plt.title('Model SLA')\n",
    "    plt.plot(x_val*sampling_time, benchmark_model_sla[:len(x_val)], linewidth=3)\n",
    "    plt.grid(True, linewidth=0.3, linestyle='-')\n",
    "    plt.show()\n",
    "\n",
    "    for container in benchmark_container_quota:\n",
    "        plt.plot(x_val*sampling_time, benchmark_container_quota[container][:len(x_val)], label=\"Allocated Cores\", linewidth=3) # label=\"container[:12]\")\n",
    "    plt.legend(loc=1)\n",
    "    plt.xlabel('Time [s]')\n",
    "    plt.ylabel('# cores')\n",
    "    if title:\n",
    "        plt.title('Allocated Cores')\n",
    "    plt.grid(True, linewidth=0.3, linestyle='-')\n",
    "    plt.show()\n",
    "    \n",
    "    # plot cores and sla\n",
    "    x_val = np.arange(len(benchmark_model_sla))\n",
    "    while len(benchmark_model_sla) < len(x_val):\n",
    "        benchmark_model_sla.append(0)\n",
    "    fig, ax1 = plt.subplots()\n",
    "    color = 'tab:red'\n",
    "    ax1.grid(True, linewidth=0.3, linestyle='-')\n",
    "    ax1.set_xlabel('Time [s]')\n",
    "    ax1.set_ylabel('SLA [s]', color=color)\n",
    "    lines = ax1.plot(x_val*sampling_time, benchmark_model_sla[:len(x_val)], ':', color=color, label=\"Model SLA\", linewidth=3)\n",
    "    ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    color = 'tab:blue'\n",
    "    ax2.set_ylabel('# cores', color=color)\n",
    "    for container in benchmark_container_quota:\n",
    "        lines += ax2.plot(x_val*sampling_time, benchmark_container_quota[container][:len(x_val)], label=\"Allocated Cores\", linewidth=3) # label=\"container[:12]\")\n",
    "    ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    if title:\n",
    "        plt.title('Allocated Cores and Model SLA')\n",
    "    labels = [l.get_label() for l in lines]\n",
    "    plt.legend(iter(lines), labels, loc=1)\n",
    "    plt.show()\n",
    "    \n",
    "    # plot cores and load\n",
    "    x_val = np.arange(len(benchmark_sent))\n",
    "    fig, ax1 = plt.subplots()\n",
    "    color = 'tab:red'\n",
    "    ax1.grid(True, linewidth=0.3, linestyle='-')\n",
    "\n",
    "    ax1.set_xlabel('Time [s]')\n",
    "    ax1.set_ylabel('Load [r/s]', color=color)\n",
    "    \n",
    "    labels = []\n",
    "    lines = ax1.plot(x_val*sampling_time, benchmark_sent, '--', linewidth=3)\n",
    "    for l, _ in enumerate(benchmark_sent[0]):\n",
    "        labels.append(\"Load \" + str(l+1))\n",
    "    ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    color = 'tab:green'\n",
    "    ax2.set_ylabel('# cores', color=color)\n",
    "    for container in benchmark_container_quota:\n",
    "        lines += ax2.plot(x_val*sampling_time, benchmark_container_quota[container][:len(x_val)], color=color, linewidth=3) # label=\"container[:12]\")\n",
    "        labels.append(\"Allocated Cores\")\n",
    "    ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    if title:\n",
    "        plt.title('Workload and Model SLA')\n",
    "    plt.legend(iter(lines), labels, loc=1)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable SLA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = \"controller/sla/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### resnet_NHWC\n",
    "- test executed with \"large.jpg\" input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"--------------- risultai tesi\")\n",
    "show_values(base_folder + \"benchmark_resnet_NHWC.out.old\", 2, aggregate=10)\n",
    "print(\"--------------- with tfserving:latest\")\n",
    "show_values(base_folder + \"benchmark_resnet_NHWC.out\", 2, aggregate=10)\n",
    "print(\"--------------- with tfserving:2.0.0\")\n",
    "show_values(base_folder + \"benchmark_resnet_NHWC_2.0.0.out\", 2, aggregate=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### vgg16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "show_values(base_folder + \"benchmark_vgg16.out\", 2, aggregate=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### googlenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"--------------- 22/04\")\n",
    "show_values(base_folder + \"benchmark_googlenet_2204.out\", 2, aggregate=10)\n",
    "print(\"--------------- control period=1s\")\n",
    "show_values(base_folder + \"benchmark_googlenet_cp1s.out\", 2, aggregate=10)\n",
    "print(\"--------------- control period=1s, rounding 3rd decimal digit\")\n",
    "show_values(base_folder + \"benchmark_googlenet_cp1s_round3.out\", 2, aggregate=10)\n",
    "print(\"--------------- control period=1s, b_c=0.15, d_c=0.11 (-0.05 rispetto originale)\")\n",
    "show_values(base_folder + \"benchmark_googlenet_cp1s_-0.05.out\", 2, aggregate=10)\n",
    "print(\"--------------- control period=5s, b_c=0.15, d_c=0.11 (-0.05 rispetto originale)\")\n",
    "show_values(base_folder + \"benchmark_googlenet_-0.05.out\", 2, aggregate=10)\n",
    "print(\"--------------- control period=1s, tf=2.0.0, rounding=3, b_c=0.10, d_c=0.08 (metà rispetto originale)\")\n",
    "show_values(base_folder + \"benchmark_googlenet_p2.out\", 2, aggregate=10)\n",
    "print(\"--------------- control period=1s, tf=2.0.0, rounding=3, b_c=0.05, d_c=0.04 (¼ rispetto originale)\")\n",
    "show_values(base_folder + \"benchmark_googlenet_p4.out\", 2, aggregate=10)\n",
    "print(\"--------------- control period=1s, tf=2.0.0, rounding=3, p_c=¼ rispetto originale, sla=1.2\")\n",
    "show_values(base_folder + \"benchmark_googlenet_p4_2.out\", 2, aggregate=10, sla=1.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### skyline_extraction min_c=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"--------------- 2204\")\n",
    "show_values(base_folder + \"benchmark_skyline_extraction_2204.out\", 2, aggregate=10)\n",
    "print(\"--------------- control period=1s, tf=2.0.0, rounding=3, p_c=½ rispetto originale\")\n",
    "show_values(base_folder + \"benchmark_skyline_extraction_p2.out\", 2, aggregate=10)\n",
    "print(\"--------------- control period=1s, tf=2.0.0, rounding=3, p_c=¼ rispetto originale\")\n",
    "show_values(base_folder + \"benchmark_skyline_extraction_p4.out\", 2, aggregate=10)\n",
    "print(\"--------------- control period=1s, tf=2.0.0, rounding=3, p_c=¼ rispetto originale, sla=1.2\")\n",
    "show_values(base_folder + \"benchmark_skyline_extraction_p4_2.out\", 2, aggregate=10, sla=1.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable Input / Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "base_folder = \"controller/input/\"\n",
    "show_values(base_folder + \"benchmark_resnet_NHWC.out\", 2, aggregate=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable Workload / req/s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "base_folder = \"controller/reqs/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### resnet_NHWC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "show_values(base_folder + \"benchmark_resnet_NHWC_small.out\", 2,aggregate=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### vgg16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "show_values(base_folder + \"benchmark_vgg16_small.out\", 2, aggregate=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### googlenet min c=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    show_values(base_folder + \"benchmark_googlenet_small.out\", 2, aggregate=10)\n",
    "except Exception as e:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### skyline_extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### min c=1, sla=0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_values(base_folder + \"benchmark_skyline_extraction_small_025.out\", 2, aggregate=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### min c=1, sla=0.35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "show_values(base_folder + \"benchmark_skyline_extraction_small_035.out\", 2, aggregate=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plt_rt(model_id, file, sampling_time, color, verbose=False, aggregate=None):\n",
    "    with open(file, 'rb') as f:\n",
    "        benchmark_data = pickle.load(f)\n",
    "    \n",
    "    benchmark_rt = list(filter(lambda rt: rt is not None, benchmark_data[0]))\n",
    "    \n",
    "    # aggregate values\n",
    "    if aggregate:\n",
    "        benchmark_aggregate = []\n",
    "        for i in range(0, len(benchmark_rt), aggregate):\n",
    "            mean = stat.mean(benchmark_rt[i:i+aggregate])\n",
    "            for _ in range(0, aggregate):\n",
    "                benchmark_aggregate.append(mean)\n",
    "        benchmark_rt = benchmark_aggregate\n",
    "    \n",
    "    benchmark_sent = benchmark_data[3]\n",
    "    benchmark_model_sla = list(map(lambda v: v*1.1, benchmark_data[4]))\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"avgs rt: \", benchmark_rt)\n",
    "        print(\"model_sla: \", benchmark_model_sla)\n",
    "    \n",
    "    # plot\n",
    "    x_val = np.arange(len(benchmark_rt))\n",
    "    slas = set(benchmark_model_sla)\n",
    "    plt.xlabel('Time [s]')\n",
    "    plt.ylabel('Response time [s]')\n",
    "    # plt.title('Response Time')\n",
    "    for i, sla in enumerate(slas):\n",
    "        plt.plot(x_val*sampling_time, [sla]*len(x_val), ':', c=color, label=\"SLA \" + str(model_id))\n",
    "    plt.plot(x_val*sampling_time, benchmark_rt, c=color, label=\"RT \" + str(model_id))\n",
    "    # plt.plot(x_val*sampling_time, [stat.mean(benchmark_rt)]*len(benchmark_rt), '--', c=color, label=\"Avg RT \" + str(model_id))\n",
    "    plt.legend(loc=1)\n",
    "    plt.grid(True, linewidth=0.3, linestyle='-')\n",
    "    \n",
    "def plt_sent(model_id, file, sampling_time, color, verbose=False):\n",
    "    with open(file, 'rb') as f:\n",
    "        benchmark_data = pickle.load(f)\n",
    "    \n",
    "    benchmark_sent = benchmark_data[3]\n",
    "    benchmark_model_sla = list(map(lambda v: v*1.1, benchmark_data[4]))\n",
    "    \n",
    "    for i,_ in enumerate(benchmark_sent):\n",
    "        for j,_ in enumerate(benchmark_sent[i]):\n",
    "            benchmark_sent[i][j] = benchmark_sent[i][j] / sampling_time\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"req sent: \", benchmark_sent)\n",
    "    \n",
    "    x_val = np.arange(len(benchmark_sent))\n",
    "    plt.xlabel('Time [s]')\n",
    "    plt.ylabel('# requests sent')\n",
    "    # plt.title('Workload')\n",
    "    lines = plt.plot(x_val*sampling_time, benchmark_sent, c=color, label=\"Load \" + str(model_id))\n",
    "    labels = []\n",
    "    for l, _ in enumerate(benchmark_sent[0]):\n",
    "        labels.append(\"load \" + str(l+1))\n",
    "    plt.legend(iter(lines), labels)\n",
    "    plt.legend(loc=1)\n",
    "    plt.grid(True, linewidth=0.3, linestyle='-')\n",
    "    \n",
    "def plt_quota(model_id, file, sampling_time, color, max_c=None, show_c=None, verbose=False, aggregate=False):\n",
    "    with open(file, 'rb') as f:\n",
    "        benchmark_data = pickle.load(f)\n",
    "    \n",
    "    benchmark_containers = benchmark_data[5]\n",
    "\n",
    "    bc_json = [response.json() for response in benchmark_containers]\n",
    "    benchmark_container_quota = {}\n",
    "    for container in bc_json[0]:\n",
    "        benchmark_container_quota[container[\"container_id\"]] = []\n",
    "    for bc in bc_json:\n",
    "        for sampled_c in bc:\n",
    "            benchmark_container_quota[sampled_c[\"container_id\"]].append(sampled_c[\"quota\"]/100000)\n",
    "            \n",
    "    if aggregate:\n",
    "        for container in benchmark_container_quota:\n",
    "            benchmark_aggregate = []\n",
    "            for i in range(0, len(benchmark_container_quota[container]), aggregate):\n",
    "                mean = stat.mean(benchmark_container_quota[container][i:i+aggregate])\n",
    "                for _ in range(0, aggregate):\n",
    "                    benchmark_aggregate.append(mean)            \n",
    "            benchmark_container_quota[container] = benchmark_aggregate\n",
    "            \n",
    "    if verbose:\n",
    "        print(\"ontainers_quota: \", benchmark_container_quota)\n",
    "    \n",
    "    for container in benchmark_container_quota:\n",
    "        x_val = np.arange(len(benchmark_container_quota[container]))\n",
    "        if show_c is not None:\n",
    "            if container in show_c:\n",
    "                plt.plot(x_val*sampling_time, benchmark_container_quota[container], c=color, label=\"Container \" + str(model_id))\n",
    "        else:\n",
    "            plt.plot(x_val*sampling_time, benchmark_container_quota[container], c=color, label=container[:12])\n",
    "\n",
    "    # print max cores\n",
    "    if max_c:\n",
    "        plt.plot(x_val*sampling_time, [max_c]*len(x_val), \"--\", c=\"m\", label=\"Max\")\n",
    "    plt.legend(loc=1)\n",
    "    plt.xlabel('Time [s]')\n",
    "    plt.ylabel('# cores')\n",
    "    # plt.title('Containers Cores')\n",
    "    plt.grid(True, linewidth=0.3, linestyle='-')\n",
    "    \n",
    "def plt_req(model_id, file, sampling_time, color, verbose=False):\n",
    "    with open(file, 'rb') as f:\n",
    "        benchmark_data = pickle.load(f)\n",
    "\n",
    "    benchmark_req = benchmark_data[2]\n",
    "    if verbose:\n",
    "        print(\"req: \", benchmark_req)\n",
    "\n",
    "    x_val = np.arange(len(benchmark_req))\n",
    "    plt.xlabel('Time [s]')\n",
    "    plt.ylabel('# requests (completed + created)')\n",
    "    # plt.title('# requests')\n",
    "    plt.plot(x_val*sampling_time, benchmark_req, c=color)\n",
    "    plt.grid(True, linewidth=0.3, linestyle='-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_stats(file, sampling_time, aggregate=False):\n",
    "    with open(file, 'rb') as f:\n",
    "        benchmark_data = pickle.load(f)\n",
    "    \n",
    "    benchmark_rt = list(filter(lambda rt: rt is not None, benchmark_data[0]))\n",
    "    benchmark_sent = benchmark_data[3]\n",
    "    benchmark_model_sla = list(map(lambda v: v*1.1, benchmark_data[4]))\n",
    "    \n",
    "    # aggregate values\n",
    "    if aggregate:\n",
    "        benchmark_aggregate = []\n",
    "        for i in range(0, len(benchmark_rt), aggregate):\n",
    "            mean = stat.mean(benchmark_rt[i:i+aggregate])\n",
    "            for _ in range(0, aggregate):\n",
    "                benchmark_aggregate.append(mean)\n",
    "        benchmark_rt = benchmark_aggregate\n",
    "        \n",
    "    \n",
    "    print(\"RT: avg: {:.4f}, max: {:.4f}, std: {:.4f}\\n\".format(stat.mean(benchmark_rt),\n",
    "                                                              max(benchmark_rt),\n",
    "                                                              stat.stdev(benchmark_rt)))\n",
    "    \n",
    "    sla = benchmark_model_sla[0]\n",
    "    violations = list(filter(lambda v: v > sla, benchmark_rt))\n",
    "    \n",
    "    print(\"Violations: {} - {:.2f}%\\n\".format(len(violations), len(violations)/len(benchmark_rt)*100))\n",
    "    \n",
    "    benchmark_containers = benchmark_data[5]\n",
    "\n",
    "    bc_json = [response.json() for response in benchmark_containers]\n",
    "    benchmark_container_quota = {}\n",
    "    for container in bc_json[0]:\n",
    "        benchmark_container_quota[container[\"container_id\"]] = []\n",
    "    for bc in bc_json:\n",
    "        for sampled_c in bc:\n",
    "            benchmark_container_quota[sampled_c[\"container_id\"]].append(sampled_c[\"quota\"]/100000)\n",
    "        \n",
    "    if aggregate:\n",
    "        for container in benchmark_container_quota:\n",
    "            benchmark_aggregate = []\n",
    "            for i in range(0, len(benchmark_container_quota[container]), aggregate):\n",
    "                mean = stat.mean(benchmark_container_quota[container][i:i+aggregate])\n",
    "                for _ in range(0, aggregate):\n",
    "                    benchmark_aggregate.append(mean)            \n",
    "            benchmark_container_quota[container] = benchmark_aggregate\n",
    "    \n",
    "    for container in benchmark_container_quota:\n",
    "        quotas = benchmark_container_quota[container]\n",
    "        active = sum(quotas)\n",
    "        print(\"Container {}:\\nactive: {:.2f}, total: {:.2f}\\n\".format(container, active, active*sampling_time))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### System Variable reqs/s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# show_values(base_folder + \"benchmark_skyline_extraction.out\",2)\n",
    "# show_values(base_folder + \"benchmark_googlenet.out\",2)\n",
    "containers = [\"1554becf604613810885d2d62ea664ca886d944905396071887e959ff4713c55\", \"79b7be123c4911e5a2eace357853bbe8886532e53601c354abccacf8f7d6ebea\"]\n",
    "output_folder = \"plot_output/\"\n",
    "for control in [\"control/high\", \"no_control_gpu_only/high\"]: #[\"control/low\", \"no_control_min_cpu\", \"no_control_max_cpu\", \"no_control_cpu_only\", \"no_control_gpu_only/low\", \"control/high\", \"no_control_gpu_only/high\"]: # \"no_control_min_cpu\", \"no_control_max_cpu\", \"no_control_cpu_only\",\n",
    "    for strategy in [\"random\", \"rr\", \"lq\", \"h1\"]: # [\"random\", \"rr\", \"lq\", \"h1\"]:\n",
    "        print(control + \" \" + strategy)\n",
    "        base_folder = \"complete/\" + control + \"/\" + strategy + \"/\"\n",
    "\n",
    "        \"\"\"\n",
    "        print(\"---------------- Skyline\")\n",
    "        show_stats(base_folder + \"benchmark_skyline_extraction.out\", 2)\n",
    "        print(\"---------------- GoogLeNet\")\n",
    "        show_stats(base_folder + \"benchmark_googlenet.out\", 2)\n",
    "        \"\"\"\n",
    "\n",
    "        print(\"++++++ AGGREGATED\")\n",
    "        print(\"---------------- Skyline\")\n",
    "        show_stats(base_folder + \"benchmark_skyline_extraction.out\", 2, aggregate=10)\n",
    "        print(\"---------------- GoogLeNet\")\n",
    "        show_stats(base_folder + \"benchmark_googlenet.out\", 2, aggregate=10)\n",
    "\n",
    "        \"\"\"\n",
    "        plt_rt(1, base_folder + \"benchmark_skyline_extraction.out\", 2, 'r')\n",
    "        plt_rt(2, base_folder + \"benchmark_googlenet.out\", 2, 'c')\n",
    "        plt.show()\n",
    "        \"\"\"\n",
    "        \n",
    "        print(\"++++++ AGGREGATED\")\n",
    "        plt_rt(1, base_folder + \"benchmark_skyline_extraction.out\", 2, 'r', aggregate=10)\n",
    "        plt_rt(2, base_folder + \"benchmark_googlenet.out\", 2, 'c', aggregate=10)\n",
    "        plt.savefig(output_folder + \"rt_\" + control.replace(\"/\", \"_\") + \"_\" + strategy  + '.pdf', bbox_inches='tight')\n",
    "        plt.show()\n",
    "\n",
    "        plt_sent(1, base_folder + \"benchmark_skyline_extraction.out\", 2, 'r')\n",
    "        plt_sent(2, base_folder + \"benchmark_googlenet.out\",2, 'c')\n",
    "        plt.savefig(output_folder + \"sent_\" + control.replace(\"/\", \"_\") + \"_\" + strategy  + '.pdf', bbox_inches='tight')\n",
    "        plt.show()\n",
    "\n",
    "        plt_req(1, base_folder + \"benchmark_skyline_extraction.out\",2, 'r')\n",
    "        plt_req(2, base_folder + \"benchmark_googlenet.out\",2, 'c')\n",
    "        plt.savefig(output_folder + \"req_\" + control.replace(\"/\", \"_\") + \"_\" + strategy  + '.pdf', bbox_inches='tight')\n",
    "        plt.show()\n",
    "\n",
    "        \"\"\"\n",
    "        plt_quota(1, base_folder + \"benchmark_skyline_extraction.out\",2, 'r', show_c=containers)\n",
    "        plt_quota(2, base_folder + \"benchmark_googlenet.out\", 2, 'c', max_c=6, show_c=containers)\n",
    "        plt.show()\n",
    "        \"\"\"\n",
    "\n",
    "        print(\"++++++ AGGREGATED\")\n",
    "        plt_quota(1, base_folder + \"benchmark_skyline_extraction.out\",2, 'r', show_c=containers, aggregate=10)\n",
    "        plt_quota(2, base_folder + \"benchmark_googlenet.out\",2, 'c', max_c=6, show_c=containers, aggregate=10)\n",
    "        plt.savefig(output_folder + \"quota_\" + control.replace(\"/\", \"_\") + \"_\" + strategy  + '.pdf', bbox_inches='tight')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
